---
sidebar_position: 0
---

# Bringing the solution up (AWS)

One-time sequence to get StereoSpot running on AWS: provision infra, generate the env file, deploy app images, then build and deploy the inference endpoint.

## Prerequisites

- **AWS credentials** configured (e.g. `AWS_PROFILE` or default credentials).
- **Node and npm** at the workspace root; run `npm ci` if you have not already.
- **Docker** for building and pushing app images (web-ui, media-worker, video-worker). When using SageMaker, the first `terraform-apply` builds and pushes a minimal stub image (GET /ping, POST /invocations) so the endpoint reaches InService; you replace it with the real image in step 4.

## The aws-infra .env file (generated)

The **`packages/aws-infra/.env`** file holds Terraform outputs (bucket names, queue URLs, ECR URLs, table names, etc.) for deploy targets, smoke tests, and runbook commands. For the **root** `.env` (Terraform variables and AWS credentials when running Terraform or AWS CLI locally), see [Environment files](/docs/intro#environment-files) in the intro.

- **Where it lives:** `packages/aws-infra/.env`
- **How you get it:** Run **`nx run aws-infra:terraform-output`** after Terraform has been applied. This target reads the current Terraform state and writes variable assignments to that file.
- **When to use it:** Deploy targets and scripts load it automatically; you do not need to source it in your shell.
- **Optional (SageMaker):** Add **`HF_TOKEN`** (your [Hugging Face token](https://huggingface.co/settings/tokens)) to the **root** `.env` and use the root target **`update-hf-token`** to write it to AWS Secrets Manager (see step 4 below).
- **Optional (YouTube URL ingest):** Enable "paste a video URL" (YouTube) by setting **`TF_VAR_enable_youtube_ingest=true`** in the **root** `.env`, applying Terraform, then running the root **`update-ytdlp-cookies`** target. See [Optional: YouTube URL ingest](#optional-youtube-url-ingest) and [Optional: yt-dlp cookies](#optional-yt-dlp-cookies) below.

If you change infra (e.g. run `terraform-apply` again), re-run `nx run aws-infra:terraform-output` to refresh it.

## Bring-up sequence

```mermaid
flowchart TB
  A[Apply aws-infra-setup] --> B[Apply aws-infra]
  B -->   C[terraform-output]
  C --> E[run-many deploy: web-ui, media-worker, video-worker]
  E --> F[sagemaker-build]
  F --> G[Wait for CodeBuild]
  G --> H[sagemaker-deploy]
```

Do these steps in order from the **workspace root**.

### 1. Apply infrastructure

Create the Terraform backend first (state bucket and lock table), then the main infra:

```bash
nx run aws-infra-setup:terraform-init
nx run aws-infra-setup:terraform-apply

nx run aws-infra:terraform-init
nx run aws-infra:terraform-plan    # optional: review
nx run aws-infra:terraform-apply
```

### 2. Generate the aws-infra .env file

Write Terraform outputs to `packages/aws-infra/.env` so deploy and runbooks can use them:

```bash
nx run aws-infra:terraform-output
```

**Optional: HTTPS and Web Push:** By default the ALB serves HTTP only. Web Push (desktop notifications when jobs finish) requires a secure context (HTTPS or localhost). To enable it: (1) Apply once so the ALB exists, run **terraform-output**. (2) Generate a certificate (e.g. [mkcert](https://github.com/FiloSottile/mkcert): on Windows, install mkcert, run `mkcert -install`, then `mkcert -cert-file alb-certificate.pem -key-file alb-private-key.pem "<ALB_DNS>"` from the project root; get `<ALB_DNS>` from **WEB_UI_ALB_DNS_NAME** in `packages/aws-infra/.env`). (3) Run **`PLATFORM=aws nx run stereo-spot:update-alb-certificates`** (optionally with `--cert-file` / `--key-file` if you used different paths). This imports the cert into ACM and sets **`TF_VAR_load_balancer_certificate_id`** in the **root** `.env`. (4) Run **terraform-apply** again; Terraform attaches the existing cert to the ALB (cert content is not in Terraform state). Use the **web_ui_url** output (**WEB_UI_URL** in `.env`); it is HTTPS when enabled. The web UI serves the VAPID public key only when the request is over HTTPS (direct or `X-Forwarded-Proto: https`), so over HTTP the notification prompt is not shown. The VAPID keypair is generated by Terraform and stored in Secrets Manager (**terraform-apply** requires **Python 3** with **cryptography** for the PEM-to-VAPID script). See [AWS runbooks - Web Push](/docs/aws/runbooks#8-job-events-pipes-and-web-push).

### 3. Deploy app images (web-ui, media-worker, video-worker)

Build Docker images, push to ECR, and trigger ECS to pull the new images:

```bash
nx run-many -t deploy --projects=web-ui,media-worker,video-worker
```

Each project’s `deploy` target depends on `aws-infra:terraform-output` and uses the ECR URLs and cluster name from `.env`.

### 4. Build and deploy the inference endpoint (SageMaker)

If you use SageMaker for inference (default), the first apply pushed a minimal stub image so the endpoint could reach InService. Build the real inference image in CodeBuild, then update the endpoint so it serves the real image.

**Set the Hugging Face token** (required for the inference container to download the model): add **`HF_TOKEN=your_token`** to the **root** `.env`, then run:

```bash
PLATFORM=aws nx run stereo-spot:update-hf-token
```

This writes the token to the secret at `HF_TOKEN_SECRET_ARN` (from `packages/aws-infra/.env`; run **terraform-output** first so that file exists).

Then:

1. **Trigger the build** (CodeBuild builds and pushes the image to ECR):

   ```bash
   nx run stereo-inference:sagemaker-build
   ```

2. **Wait** for the CodeBuild job to finish (check the AWS CodeBuild console or pipeline).

3. **Update the SageMaker endpoint** to use the new image:

   ```bash
   nx run stereo-inference:sagemaker-deploy
   ```

If you use **HTTP inference** instead of SageMaker, skip this step and run your own inference server; point the video-worker at it via `INFERENCE_HTTP_URL` (set in Terraform / `.env`).

### Optional: YouTube URL ingest

**YouTube URL ingest** (paste a video URL on the job page instead of uploading a file) is **optional** and driven by the Terraform variable **`TF_VAR_enable_youtube_ingest`** (set in the **root** `.env`).

- **When `TF_VAR_enable_youtube_ingest=true`:** Terraform creates the ingest SQS queue, the yt-dlp cookies secret (placeholder), and passes `INGEST_QUEUE_URL` to web-ui and media-worker. The web UI shows "Or paste a video URL"; the media-worker runs the ingest loop. Push the cookie file to Secrets Manager with the root **`update-ytdlp-cookies`** target (see [Optional: yt-dlp cookies](#optional-yt-dlp-cookies) below).
- **When the variable is false or unset:** No ingest queue or cookies secret is created; `INGEST_QUEUE_URL` is not set. The web UI shows only the upload option; the media-worker skips the ingest loop.

To enable the feature: set **`TF_VAR_enable_youtube_ingest=true`** in the **root** `.env`, run **terraform-apply** and **terraform-output**, then run **`PLATFORM=aws nx run stereo-spot:update-ytdlp-cookies`** (optionally with `--cookies-file <path>`). The script also sets **`TF_VAR_enable_youtube_ingest=true`** in root `.env` after pushing cookies. Re-deploy web-ui and media-worker. To disable: set the variable to `false`, apply and output, then re-deploy.

### Optional: yt-dlp cookies

When YouTube URL ingest is enabled (**`TF_VAR_enable_youtube_ingest=true`**) and users paste a YouTube URL, the media-worker downloads the video with yt-dlp. Some videos trigger YouTube’s "Sign in to confirm you're not a bot" when no cookies are sent. To fix that, use a **dedicated** Google/YouTube account (not your personal one), export its cookies, and store them in Secrets Manager.

1. **Export cookies (Netscape format)**  
   - In Chrome: install [Get cookies.txt LOCALLY](https://chromewebstore.google.com/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc). Log in to YouTube with the dedicated account, open the extension, export cookies for the current site in Netscape / cookies.txt format.  
   - Save the file as **`ytdlp_cookies.txt`** at the **project root** (default; you can pass a different path with `--cookies-file`). Do **not** commit it (`ytdlp_cookies.txt` is in `.gitignore`).

2. **Enable feature and apply**  
   - Set **`TF_VAR_enable_youtube_ingest=true`** in the **root** `.env`, run **terraform-apply** (so the ytdlp-cookies secret exists), then **terraform-output**. The file **`packages/aws-infra/.env`** will contain **`YTDLP_COOKIES_SECRET_ARN`**.

3. **Push to Secrets Manager**  
   - From the workspace root run:  
     ```bash
     PLATFORM=aws nx run stereo-spot:update-ytdlp-cookies
     ```  
   The script sets **`TF_VAR_enable_youtube_ingest=true`** in root `.env` after pushing. Optionally use **`--cookies-file <path>`** for a different cookies file. The media-worker fetches the secret at startup and passes the cookies to yt-dlp.

Cookies expire; re-export and re-run the **update-ytdlp-cookies** target periodically if downloads start failing again.

## Verify

- Open the web UI (ALB URL from Terraform or `.env`).
- Create a job, upload a video, and confirm the pipeline runs (chunking → segment processing → reassembly).
- Optionally run the data-plane smoke test: `nx run aws-adapters:smoke-test` (with `.env` loaded).

## See also

- [AWS build and deploy](/docs/aws/build-and-deploy) — Deploy order and manual steps for updates.
- [AWS runbooks](/docs/aws/runbooks) — Recovery and operations (DLQ, scaling, SageMaker update).
- [AWS infrastructure](/docs/aws/infrastructure) — Terraform layout and Nx targets.
